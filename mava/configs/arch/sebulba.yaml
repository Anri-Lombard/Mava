# note n_envs *must* be divisible by n_learners (length of learner_device_ids)
# because we do a sharded device put of experience generated by n_envs so the shape is
# (n_envs, ...) this needs to be splittable among the learners
local_n_envs: 16  # number of envs in async env

n_evals_envs: 4  # number of eval envs
n_eval_episodes: 8  # number of episodes to evaluate for

n_threads_per_actor: 1  # num of different threads/env batches per actor
actor_device_ids: [0] # ids of actor devices
learner_device_ids: [0] # ids of learner devices

world_size: 0 # total number of pods / servers. eg. 2 full TPUs or 2 full GPUs.
distributed: False # whether more than one pod is available. eg. world_size > 1
concurrency: False # whether actor and learner should run concurrently
async_envs: True # "whether to use async vector or sync vector envs"

eval_period: 1_000 # how many learner steps between evaluations
param_upd_freq: 1 # how many learner steps between sending learner_params -> actor params
log_frequency: 10 # how often to log to neptune / the terminal (in learner update steps)
checkpoint_frequency: 20 # how checkpoint model (in learner update steps)

load_model: ~ # path to load model from, if None then randomly initialise"
save_model: True # whether to checkpoint the model
local_rank: 0 # index of current jax process
