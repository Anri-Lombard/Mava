# note n_envs *must* be divisible by n_learners (length of learner_device_ids)
# because we do a sharded device put of experience generated by n_envs so the shape is
# (n_envs, ...) this needs to be splittable among the learners
arch_name: "sebulba"
num_envs: 16  # number of envs per thread

# --- Evaluation ---
evaluation_greedy: False # Evaluate the policy greedily. If True the policy will select
  # an action which corresponds to the greatest logit. If false, the policy will sample
  # from the logits.
num_eval_episodes: 8 # Number of episodes to evaluate per evaluation.
num_evaluation: 250 # Number of evenly spaced evaluations to perform during training.
absolute_metric: True # Whether the absolute metric should be computed. For more details

n_threads_per_executor: 1  # num of different threads/env batches per actor
executor_device_ids: [0] # ids of actor devices
learner_device_ids: [0] # ids of learner devices

world_size: 0 # total number of pods / servers. eg. 2 full TPUs or 2 full GPUs.
concurrency: False # whether actor and learner should run concurrently
async_envs: True # "whether to use async vector or sync vector envs"

eval_period: 1_000 # how many learner steps between evaluations
param_upd_freq: 1 # how many learner steps between sending learner_params -> actor params
log_frequency: 10 # how often to log to neptune / the terminal (in learner update steps)
checkpoint_frequency: 20 # how checkpoint model (in learner update steps)

load_model: ~ # path to load model from, if None then randomly initialise"
save_model: True # whether to checkpoint the model
local_rank: 0 # index of current jax process
