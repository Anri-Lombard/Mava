# ---MLP Networks---
actor_network:
  pre_torso:
    _target_: mava.networks.MLPTorso
    layer_sizes: [128, 128]
    use_layer_norm: False
    activation: relu

action_head:
  _target_: mava.networks.DiscreteActionHead # [DiscreteActionHead, NormalTanhDistributionHead, ContinuousActionHead]
  # Needed for ContinuousActionHead:
  sigma_min: -4.0
  sigma_max: 1.0

critic_network:
  pre_torso:
    _target_: mava.networks.MLPTorso
    layer_sizes: [128, 128]
    use_layer_norm: False
    activation: relu
