{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uCEQLS3zZUn"
      },
      "source": [
        "# JAX Mava Quickstart Notebook\n",
        "<img src=\"https://raw.githubusercontent.com/instadeepai/Mava/develop/docs/images/mava.png\" />\n",
        "\n",
        "### This notebook offers a simple initiation to [Mava](https://github.com/instadeepai/Mava) through the illustration of training a multi-agent PPO (MAPPO) using the Robot Warehouse environment as an example.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/instadeepai/Mava/blob/develop/quickstart.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Requirements\n",
        "\n",
        "We start by installing and importing the necessary packages."
      ],
      "metadata": {
        "id": "LYmyi-lU3a-b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Install Mava\n",
        "! pip install git+https://github.com/instadeepai/mava.git@feat/pure-jax-mava\n",
        "! pip install \"jax[cuda12_pip]<=0.4.13\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
      ],
      "metadata": {
        "id": "5l-eEkH-2f0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Import required packages. (Run Cell)\n",
        "\n",
        "from typing import Any, Callable, Dict, Sequence, Tuple\n",
        "from colorama import Fore, Style\n",
        "\n",
        "import optax\n",
        "from optax._src.base import OptState\n",
        "import chex\n",
        "import distrax\n",
        "import flax.linen as nn\n",
        "from flax import struct\n",
        "from flax.core.frozen_dict import FrozenDict\n",
        "from flax.linen.initializers import constant, orthogonal\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "# Env requirements\n",
        "import jumanji\n",
        "from jumanji.env import Environment\n",
        "from jumanji.environments.routing.robot_warehouse import Observation, State\n",
        "from jumanji.environments.routing.robot_warehouse.generator import RandomGenerator\n",
        "from jumanji import specs\n",
        "from jumanji.wrappers import AutoResetWrapper\n",
        "\n",
        "# Mava Helpful functions and types\n",
        "from mava.utils.jax import merge_leading_dims\n",
        "from mava.utils.timing_utils import TimeIt\n",
        "from mava.wrappers.jumanji import (\n",
        "    AgentIDWrapper,\n",
        "    LogWrapper,\n",
        "    ObservationGlobalState,\n",
        "    RwareMultiAgentWithGlobalStateWrapper,\n",
        ")\n",
        "from mava.types import ExperimentOutput, LearnerState, PPOTransition\n",
        "from mava.evaluator import evaluator_setup\n",
        "\n",
        "# Plot requirements\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "sns.set_style(\"white\")\n",
        "sns.color_palette(\"colorblind\")\n",
        "import time"
      ],
      "metadata": {
        "id": "FjXA8JyI1_YW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer\n",
        "\n"
      ],
      "metadata": {
        "id": "9omksZSH6htZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network\n",
        "\n",
        "Initially, we construct the ActorCritic network using components from the Flax library. The application of the actor-critic's function will then be \"vmapped\" across distinct agents, with the in_axes parameter applied solely to the observation and not the network parameters."
      ],
      "metadata": {
        "id": "JaIw_5YaUSAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActorCritic(nn.Module):\n",
        "    \"\"\"Actor Critic Network.\"\"\"\n",
        "\n",
        "    action_dim: Sequence[int]\n",
        "\n",
        "    @nn.compact\n",
        "    def __call__(self, observation: Observation) -> Tuple[distrax.Categorical, chex.Array]:\n",
        "        \"\"\"Forward pass.\"\"\"\n",
        "        x = observation.agents_view\n",
        "\n",
        "        actor_output = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(x)\n",
        "        actor_output = nn.relu(actor_output)\n",
        "        actor_output = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(\n",
        "            actor_output\n",
        "        )\n",
        "        actor_output = nn.relu(actor_output)\n",
        "        actor_output = nn.Dense(\n",
        "            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)\n",
        "        )(actor_output)\n",
        "\n",
        "        masked_logits = jnp.where(\n",
        "            observation.action_mask,\n",
        "            actor_output,\n",
        "            jnp.finfo(jnp.float32).min,\n",
        "        )\n",
        "        actor_policy = distrax.Categorical(logits=masked_logits)\n",
        "\n",
        "        y = observation.global_state\n",
        "\n",
        "        critic_output = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(y)\n",
        "        critic_output = nn.relu(critic_output)\n",
        "        critic_output = nn.Dense(64, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))(\n",
        "            critic_output\n",
        "        )\n",
        "        critic_output = nn.relu(critic_output)\n",
        "        critic_output = nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))(\n",
        "            critic_output\n",
        "        )\n",
        "\n",
        "        return actor_policy, jnp.squeeze(critic_output, axis=-1)"
      ],
      "metadata": {
        "id": "Sss6opmC6lmp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learner Function\n",
        "The get_learner_fn function returns a learner function returns `ExperimentOutput`, encapsulating updated learner state, episode information, and loss metrics. This function is essential in training the MAPPO."
      ],
      "metadata": {
        "id": "IFraNFqY6s7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_learner_fn(\n",
        "    env: jumanji.Environment, apply_fn: Callable, update_fn: Callable, config: Dict\n",
        ") -> Callable:\n",
        "    \"\"\"Get the learner function.\"\"\"\n",
        "\n",
        "    def _update_step(learner_state: LearnerState, _: Any) -> Tuple[LearnerState, Tuple]:\n",
        "        \"\"\"A single update of the network.\n",
        "\n",
        "        This function steps the environment and records the trajectory batch for\n",
        "        training. It then calculates advantages and targets based on the recorded\n",
        "        trajectory and updates the actor and critic networks based on the calculated\n",
        "        losses.\n",
        "\n",
        "        Args:\n",
        "            learner_state (NamedTuple):\n",
        "                - params (FrozenDict): The current model parameters.\n",
        "                - opt_state (OptState): The current optimizer state.\n",
        "                - rng (PRNGKey): The random number generator state.\n",
        "                - env_state (State): The environment state.\n",
        "                - last_timestep (TimeStep): The last timestep in the current trajectory.\n",
        "            _ (Any): The current metrics info.\n",
        "        \"\"\"\n",
        "\n",
        "        def _env_step(learner_state: LearnerState, _: Any) -> Tuple[LearnerState, PPOTransition]:\n",
        "            \"\"\"Step the environment.\"\"\"\n",
        "            params, opt_state, rng, env_state, last_timestep = learner_state\n",
        "\n",
        "            # SELECT ACTION\n",
        "            rng, policy_rng = jax.random.split(rng)\n",
        "            actor_policy, value = apply_fn(params, last_timestep.observation)\n",
        "            action = actor_policy.sample(seed=policy_rng)\n",
        "            log_prob = actor_policy.log_prob(action)\n",
        "\n",
        "            # STEP ENVIRONMENT\n",
        "            env_state, timestep = jax.vmap(env.step, in_axes=(0, 0))(env_state, action)\n",
        "\n",
        "            # LOG EPISODE METRICS\n",
        "            done, reward = jax.tree_util.tree_map(\n",
        "                lambda x: jnp.repeat(x, config[\"num_agents\"]).reshape(config[\"num_envs\"], -1),\n",
        "                (timestep.last(), timestep.reward),\n",
        "            )\n",
        "            info = {\n",
        "                \"episode_return\": env_state.episode_return_info,\n",
        "                \"episode_length\": env_state.episode_length_info,\n",
        "            }\n",
        "\n",
        "            transition = PPOTransition(\n",
        "                done, action, value, reward, log_prob, last_timestep.observation, info\n",
        "            )\n",
        "            learner_state = LearnerState(params, opt_state, rng, env_state, timestep)\n",
        "            return learner_state, transition\n",
        "\n",
        "        # STEP ENVIRONMENT FOR ROLLOUT LENGTH\n",
        "        learner_state, traj_batch = jax.lax.scan(\n",
        "            _env_step, learner_state, None, config[\"rollout_length\"]\n",
        "        )\n",
        "\n",
        "        # CALCULATE ADVANTAGE\n",
        "        params, opt_state, rng, env_state, last_timestep = learner_state\n",
        "        _, last_val = apply_fn(params, last_timestep.observation)\n",
        "\n",
        "        def _calculate_gae(\n",
        "            traj_batch: PPOTransition, last_val: chex.Array\n",
        "        ) -> Tuple[chex.Array, chex.Array]:\n",
        "            \"\"\"Calculate the GAE.\"\"\"\n",
        "\n",
        "            def _get_advantages(gae_and_next_value: Tuple, transition: PPOTransition) -> Tuple:\n",
        "                \"\"\"Calculate the GAE for a single transition.\"\"\"\n",
        "                gae, next_value = gae_and_next_value\n",
        "                done, value, reward = (\n",
        "                    transition.done,\n",
        "                    transition.value,\n",
        "                    transition.reward,\n",
        "                )\n",
        "                delta = reward + config[\"gamma\"] * next_value * (1 - done) - value\n",
        "                gae = delta + config[\"gamma\"] * config[\"gae_lambda\"] * (1 - done) * gae\n",
        "                return (gae, value), gae\n",
        "\n",
        "            _, advantages = jax.lax.scan(\n",
        "                _get_advantages,\n",
        "                (jnp.zeros_like(last_val), last_val),\n",
        "                traj_batch,\n",
        "                reverse=True,\n",
        "                unroll=16,\n",
        "            )\n",
        "            return advantages, advantages + traj_batch.value\n",
        "\n",
        "        advantages, targets = _calculate_gae(traj_batch, last_val)\n",
        "\n",
        "        def _update_epoch(update_state: Tuple, _: Any) -> Tuple:\n",
        "            \"\"\"Update the network for a single epoch.\"\"\"\n",
        "\n",
        "            def _update_minibatch(train_state: Tuple, batch_info: Tuple) -> Tuple:\n",
        "                \"\"\"Update the network for a single minibatch.\"\"\"\n",
        "                params, opt_state = train_state\n",
        "                traj_batch, advantages, targets = batch_info\n",
        "\n",
        "                def _loss_fn(\n",
        "                    params: FrozenDict,\n",
        "                    opt_state: OptState,\n",
        "                    traj_batch: PPOTransition,\n",
        "                    gae: chex.Array,\n",
        "                    targets: chex.Array,\n",
        "                ) -> Tuple:\n",
        "                    \"\"\"Calculate the loss.\"\"\"\n",
        "                    # RERUN NETWORK\n",
        "                    actor_policy, value = apply_fn(params, traj_batch.obs)\n",
        "                    log_prob = actor_policy.log_prob(traj_batch.action)\n",
        "\n",
        "                    # CALCULATE VALUE LOSS\n",
        "                    value_pred_clipped = traj_batch.value + (value - traj_batch.value).clip(\n",
        "                        -config[\"clip_eps\"], config[\"clip_eps\"]\n",
        "                    )\n",
        "                    value_losses = jnp.square(value - targets)\n",
        "                    value_losses_clipped = jnp.square(value_pred_clipped - targets)\n",
        "                    value_loss = 0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()\n",
        "\n",
        "                    # CALCULATE ACTOR LOSS\n",
        "                    ratio = jnp.exp(log_prob - traj_batch.log_prob)\n",
        "                    gae = (gae - gae.mean()) / (gae.std() + 1e-8)\n",
        "                    loss_actor1 = ratio * gae\n",
        "                    loss_actor2 = (\n",
        "                        jnp.clip(\n",
        "                            ratio,\n",
        "                            1.0 - config[\"clip_eps\"],\n",
        "                            1.0 + config[\"clip_eps\"],\n",
        "                        )\n",
        "                        * gae\n",
        "                    )\n",
        "                    loss_actor = -jnp.minimum(loss_actor1, loss_actor2)\n",
        "                    loss_actor = loss_actor.mean()\n",
        "                    entropy = actor_policy.entropy().mean()\n",
        "\n",
        "                    total_loss = (\n",
        "                        loss_actor + config[\"vf_coef\"] * value_loss - config[\"ent_coef\"] * entropy\n",
        "                    )\n",
        "                    return total_loss, (value_loss, loss_actor, entropy)\n",
        "\n",
        "                grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)\n",
        "                loss_info, grads = grad_fn(params, opt_state, traj_batch, advantages, targets)\n",
        "\n",
        "                # Compute the parallel mean (pmean) over the batch.\n",
        "                # This calculation is inspired by the Anakin architecture demo notebook.\n",
        "                # available at https://tinyurl.com/26tdzs5x\n",
        "                # This pmean could be a regular mean as the batch axis is on the same device.\n",
        "                grads, loss_info = jax.lax.pmean((grads, loss_info), axis_name=\"batch\")\n",
        "                # pmean over devices.\n",
        "                grads, loss_info = jax.lax.pmean((grads, loss_info), axis_name=\"device\")\n",
        "\n",
        "                updates, new_opt_state = update_fn(grads, opt_state)\n",
        "                new_params = optax.apply_updates(params, updates)\n",
        "\n",
        "                return (new_params, new_opt_state), loss_info\n",
        "\n",
        "            params, opt_state, traj_batch, advantages, targets, rng = update_state\n",
        "            rng, shuffle_rng = jax.random.split(rng)\n",
        "\n",
        "            # SHUFFLE MINIBATCHES\n",
        "            batch_size = config[\"rollout_length\"] * config[\"num_envs\"]\n",
        "            permutation = jax.random.permutation(shuffle_rng, batch_size)\n",
        "            batch = (traj_batch, advantages, targets)\n",
        "            batch = jax.tree_util.tree_map(lambda x: merge_leading_dims(x, 2), batch)\n",
        "            shuffled_batch = jax.tree_util.tree_map(\n",
        "                lambda x: jnp.take(x, permutation, axis=0), batch\n",
        "            )\n",
        "            minibatches = jax.tree_util.tree_map(\n",
        "                lambda x: jnp.reshape(x, [config[\"num_minibatches\"], -1] + list(x.shape[1:])),\n",
        "                shuffled_batch,\n",
        "            )\n",
        "\n",
        "            # UPDATE MINIBATCHES\n",
        "            (params, opt_state), loss_info = jax.lax.scan(\n",
        "                _update_minibatch, (params, opt_state), minibatches\n",
        "            )\n",
        "\n",
        "            update_state = (params, opt_state, traj_batch, advantages, targets, rng)\n",
        "            return update_state, loss_info\n",
        "\n",
        "        update_state = (params, opt_state, traj_batch, advantages, targets, rng)\n",
        "\n",
        "        # UPDATE EPOCHS\n",
        "        update_state, loss_info = jax.lax.scan(\n",
        "            _update_epoch, update_state, None, config[\"ppo_epochs\"]\n",
        "        )\n",
        "\n",
        "        params, opt_state, traj_batch, advantages, targets, rng = update_state\n",
        "        learner_state = LearnerState(params, opt_state, rng, env_state, last_timestep)\n",
        "        metric = traj_batch.info\n",
        "        return learner_state, (metric, loss_info)\n",
        "\n",
        "    def learner_fn(learner_state: LearnerState) -> ExperimentOutput:\n",
        "        \"\"\"Learner function.\n",
        "\n",
        "        This function represents the learner, it updates the network parameters\n",
        "        by iteratively applying the `_update_step` function for a fixed number of\n",
        "        updates. The `_update_step` function is vectorized over a batch of inputs.\n",
        "\n",
        "        Args:\n",
        "            learner_state (NamedTuple):\n",
        "                - params (FrozenDict): The initial model parameters.\n",
        "                - opt_state (OptState): The initial optimizer state.\n",
        "                - rng (chex.PRNGKey): The random number generator state.\n",
        "                - env_state (LogEnvState): The environment state.\n",
        "                - timesteps (TimeStep): The initial timestep in the initial trajectory.\n",
        "        \"\"\"\n",
        "\n",
        "        batched_update_step = jax.vmap(_update_step, in_axes=(0, None), axis_name=\"batch\")\n",
        "\n",
        "        learner_state, (metric, loss_info) = jax.lax.scan(\n",
        "            batched_update_step, learner_state, None, config[\"num_updates_per_eval\"]\n",
        "        )\n",
        "        total_loss, (value_loss, loss_actor, entropy) = loss_info\n",
        "        return ExperimentOutput(\n",
        "            learner_state=learner_state,\n",
        "            episodes_info=metric,\n",
        "            total_loss=total_loss,\n",
        "            value_loss=value_loss,\n",
        "            loss_actor=loss_actor,\n",
        "            entropy=entropy,\n",
        "        )\n",
        "\n",
        "    return learner_fn\n"
      ],
      "metadata": {
        "id": "4VVjKmgW64Ct"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer Setup\n",
        "The learner setup initializes components for training: the learner function, neural network, optimizer, environment, and states. It creates a function for learning, employs parallel processing over the cores for efficiency, and sets up initial states."
      ],
      "metadata": {
        "id": "4idyWUhW68oS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def learner_setup(\n",
        "    env: Environment, rngs: chex.Array, config: Dict\n",
        ") -> Tuple[callable, ActorCritic, LearnerState]:\n",
        "    \"\"\"Initialise learner_fn, network, optimiser, environment and states.\"\"\"\n",
        "    # Get available TPU cores.\n",
        "    n_devices = len(jax.devices())\n",
        "    # Get number of actions and agents.\n",
        "    num_actions = int(env.action_spec().num_values[0])\n",
        "    num_agents = env.action_spec().shape[0]\n",
        "    config[\"num_agents\"] = num_agents\n",
        "    # PRNG keys.\n",
        "    rng, rng_p = rngs\n",
        "\n",
        "    # Define network and optimiser.\n",
        "    network = ActorCritic(num_actions)\n",
        "    optim = optax.chain(\n",
        "        optax.clip_by_global_norm(config[\"max_grad_norm\"]),\n",
        "        optax.adam(config[\"lr\"], eps=1e-5),\n",
        "    )\n",
        "\n",
        "    # Initialise observation.\n",
        "    obs = env.observation_spec().generate_value()\n",
        "    # Select only obs for a single agent.\n",
        "    init_x = ObservationGlobalState(\n",
        "        agents_view=obs.agents_view[0],\n",
        "        action_mask=obs.action_mask[0],\n",
        "        global_state=obs.global_state,\n",
        "        step_count=obs.step_count[0],\n",
        "    )\n",
        "    init_x = jax.tree_util.tree_map(lambda x: x[None, ...], init_x)\n",
        "\n",
        "    # initialise params and optimiser state.\n",
        "    params = network.init(rng_p, init_x)\n",
        "    opt_state = optim.init(params)\n",
        "\n",
        "    # Vmap network apply function over number of agents.\n",
        "    vmapped_network_apply_fn = jax.vmap(\n",
        "        network.apply,\n",
        "        in_axes=(None, ObservationGlobalState(1, 1, None, 1)),\n",
        "        out_axes=(1, 1),\n",
        "    )\n",
        "\n",
        "    # Get batched iterated update and replicate it to pmap it over cores.\n",
        "    learn = get_learner_fn(env, vmapped_network_apply_fn, optim.update, config)\n",
        "    learn = jax.pmap(learn, axis_name=\"device\")\n",
        "\n",
        "    # Broadcast params and optimiser state to cores and batch.\n",
        "    broadcast = lambda x: jnp.broadcast_to(x, (n_devices, config[\"update_batch_size\"]) + x.shape)\n",
        "    params = jax.tree_map(broadcast, params)\n",
        "    opt_state = jax.tree_map(broadcast, opt_state)\n",
        "\n",
        "    # Initialise environment states and timesteps.\n",
        "    rng, *env_rngs = jax.random.split(\n",
        "        rng, n_devices * config[\"update_batch_size\"] * config[\"num_envs\"] + 1\n",
        "    )\n",
        "    env_states, timesteps = jax.vmap(env.reset, in_axes=(0))(\n",
        "        jnp.stack(env_rngs),\n",
        "    )\n",
        "\n",
        "    # Split rngs for each core.\n",
        "    rng, *step_rngs = jax.random.split(rng, n_devices * config[\"update_batch_size\"] + 1)\n",
        "    # Add dimension to pmap over.\n",
        "    reshape_step_rngs = lambda x: x.reshape((n_devices, config[\"update_batch_size\"]) + x.shape[1:])\n",
        "    step_rngs = reshape_step_rngs(jnp.stack(step_rngs))\n",
        "    reshape_states = lambda x: x.reshape(\n",
        "        (n_devices, config[\"update_batch_size\"], config[\"num_envs\"]) + x.shape[1:]\n",
        "    )\n",
        "    env_states = jax.tree_util.tree_map(reshape_states, env_states)\n",
        "    timesteps = jax.tree_util.tree_map(reshape_states, timesteps)\n",
        "\n",
        "    init_learner_state = LearnerState(params, opt_state, step_rngs, env_states, timesteps)\n",
        "    return learn, network, init_learner_state"
      ],
      "metadata": {
        "id": "eWjNSGvZ7ALw"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Rendering and logging tools"
      ],
      "metadata": {
        "id": "GefUs8Yd7EJt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rendering\n",
        "The `render_one_episode` function simulates and visualizes one episode using a trained MAPPO model that will be passed to the function using `params`."
      ],
      "metadata": {
        "id": "Vkd95_VpYJf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def render_one_episode(config, params, seed) -> Tuple:\n",
        "    \"\"\"Render one episode using trained MAPPO\"\"\"\n",
        "\n",
        "    def env_step(episode_state, apply_fn):\n",
        "        \"\"\"Step the environment.\"\"\"\n",
        "        # PRNG keys.\n",
        "        rng, env_state, last_timestep, step_count_, return_, states = episode_state\n",
        "\n",
        "        # Select action.\n",
        "        rng, _rng = jax.random.split(rng)\n",
        "        pi, _ = apply_fn(params, last_timestep.observation)\n",
        "\n",
        "        if config[\"evaluation_greedy\"]:\n",
        "            action = pi.mode()\n",
        "        else:\n",
        "            action = pi.sample(seed=_rng)\n",
        "\n",
        "        # Step environment.\n",
        "        env_state, timestep = env.step(env_state, action)\n",
        "\n",
        "        # Log episode metrics.\n",
        "        return_ += timestep.reward\n",
        "        step_count_ += 1\n",
        "        states.append(env_state)\n",
        "        episode_state = (rng, env_state, timestep, step_count_, return_, states)\n",
        "        return episode_state\n",
        "\n",
        "    # Network\n",
        "    env = jumanji.make(config[\"env_name\"])\n",
        "    env = RwareMultiAgentWithGlobalStateWrapper(env)\n",
        "    num_actions = int(env.action_spec().num_values[0])\n",
        "    network = ActorCritic(num_actions)\n",
        "    vmapped_network_apply_fn = jax.vmap(\n",
        "        network.apply,\n",
        "        in_axes=(None, ObservationGlobalState(0, 0, None, 0)),\n",
        "    )\n",
        "\n",
        "    # Rng\n",
        "    rng = jax.random.PRNGKey(seed)\n",
        "\n",
        "    # Build and Initialise env\n",
        "    state, timestep=env.reset(rng)\n",
        "\n",
        "    states = []\n",
        "    episode_state = (rng, state, timestep, 0, 0, states)\n",
        "    while not episode_state[2].last():\n",
        "      episode_state= env_step(episode_state, vmapped_network_apply_fn)\n",
        "\n",
        "    # Record and print results\n",
        "    rng, env_state, last_timestep, step_count_, return_, states = episode_state\n",
        "    env.animate(states=states, save_path=\"./rware.gif\")\n",
        "    print(f\"{Fore.CYAN}{Style.BRIGHT}EPISODE RETURN: {return_}{Style.RESET_ALL}\")\n",
        "    print(f\"{Fore.CYAN}{Style.BRIGHT}EPISODE LENGTH:{step_count_}{Style.RESET_ALL}\")"
      ],
      "metadata": {
        "id": "XKQofjLL7T3B"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Logging:\n",
        "The `plot_performance` function visualizes the performance of the algorithm, this plot will be refreshed each time evaluation interval happens!"
      ],
      "metadata": {
        "id": "N2pDHF1Q8Cn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_performance(metrics, ep_returns, start_time):\n",
        "      plt.figure(figsize=(8, 4))\n",
        "      clear_output(wait=True)\n",
        "\n",
        "      ep_returns.append(metrics.episodes_info[\"episode_return\"].mean())\n",
        "      # Plot the data\n",
        "      plt.plot(np.linspace(0, (time.time()-start_time)/ 60.0, len(list(ep_returns))),list(ep_returns))\n",
        "      plt.xlabel('Run Time [Minutes]')\n",
        "      plt.ylabel('Episode Return')\n",
        "      plt.title(f'Robotic Warehouse with 4 Agents')\n",
        "      # Show the plot\n",
        "      plt.show()\n",
        "      return ep_returns"
      ],
      "metadata": {
        "id": "OwkZqb8y8GYG"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exeperiment Setup (function and Hyperparameters)\n"
      ],
      "metadata": {
        "id": "GLLqQgn1754J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `run_experiment` function executes an experiment by training MAPPO and evaluating its performance:\n",
        "\n",
        "The function creates environments, sets up the learner and evaluator, and calculates the total timesteps for training. It then trains the model, evaluates it, plots the performance, and updates the learner state. After completing the specified number of evaluations, it returns the trained parameters."
      ],
      "metadata": {
        "id": "CwFYuKpfZyx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(config: Dict) -> None:\n",
        "    \"\"\"Runs experiment.\"\"\"\n",
        "    # Create envs\n",
        "    env = jumanji.make(config[\"env_name\"])\n",
        "    env = RwareMultiAgentWithGlobalStateWrapper(env)\n",
        "    env = AutoResetWrapper(env)\n",
        "    env = LogWrapper(env)\n",
        "    eval_env = jumanji.make(config[\"env_name\"])\n",
        "    eval_env = RwareMultiAgentWithGlobalStateWrapper(eval_env)\n",
        "\n",
        "    # PRNG keys.\n",
        "    rng, rng_e, rng_p = jax.random.split(jax.random.PRNGKey(config[\"seed\"]), num=3)\n",
        "\n",
        "    # Setup learner.\n",
        "    learn, network, learner_state = learner_setup(env, (rng, rng_p), config)\n",
        "\n",
        "    # Setup evaluator.\n",
        "    evaluator, _,(trained_params, eval_rngs) = evaluator_setup(\n",
        "        eval_env=eval_env,\n",
        "        rng_e=rng_e,\n",
        "        network=network,\n",
        "        params=learner_state.params,\n",
        "        config=config,\n",
        "        centralised_critic=True,\n",
        "    )\n",
        "\n",
        "    # Calculate total timesteps.\n",
        "    n_devices = len(jax.devices())\n",
        "    config[\"num_updates_per_eval\"] = config[\"num_updates\"] // config[\"num_evaluation\"]\n",
        "    timesteps_per_training = (\n",
        "        n_devices\n",
        "        * config[\"num_updates_per_eval\"]\n",
        "        * config[\"rollout_length\"]\n",
        "        * config[\"update_batch_size\"]\n",
        "        * config[\"num_envs\"]\n",
        "    )\n",
        "\n",
        "    # Run experiment for a total number of evaluations.\n",
        "    start_time=time.time()\n",
        "    ep_returns=[]\n",
        "    for i in range(config[\"num_evaluation\"]):\n",
        "        # Train.\n",
        "        with TimeIt(\n",
        "            tag=(\"COMPILATION\" if i == 0 else \"EXECUTION\"),\n",
        "            environment_steps=timesteps_per_training,\n",
        "        ):\n",
        "            learner_output = learn(learner_state)\n",
        "            jax.block_until_ready(learner_output)\n",
        "\n",
        "\n",
        "        # Prepare for evaluation.\n",
        "        trained_params = jax.tree_util.tree_map(\n",
        "            lambda x: x[:, 0, ...], learner_output.learner_state.params\n",
        "        )\n",
        "        rng_e, *eval_rngs = jax.random.split(rng_e, n_devices + 1)\n",
        "        eval_rngs = jnp.stack(eval_rngs)\n",
        "        eval_rngs = eval_rngs.reshape(n_devices, -1)\n",
        "\n",
        "        # Evaluate.\n",
        "        evaluator_output = evaluator(trained_params, eval_rngs)\n",
        "        jax.block_until_ready(evaluator_output)\n",
        "        ep_returns=plot_performance(evaluator_output, ep_returns, start_time)\n",
        "\n",
        "        # Update runner state to continue training.\n",
        "        learner_state = learner_output.learner_state\n",
        "\n",
        "    # Return trained params to be used for rendering or testing.\n",
        "    trained_params= jax.tree_util.tree_map(\n",
        "            lambda x: x[0, 0, ...], learner_output.learner_state.params\n",
        "        )\n",
        "    return trained_params"
      ],
      "metadata": {
        "id": "3kSjCFr_-xE3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Config\n",
        "\n",
        "The provided config dictionary sets various hyperparameters for the experiment"
      ],
      "metadata": {
        "id": "OHgQbXfY8LqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "        \"lr\": 2.5e-4,\n",
        "        \"update_batch_size\": 2,\n",
        "        \"rollout_length\": 128, # Number of steps per episode rollout.\n",
        "        \"num_updates\": 1000, # Total number of training updates.\n",
        "        \"num_envs\": 64,  # Number of parallel environments.\n",
        "        \"ppo_epochs\": 4,\n",
        "        \"num_minibatches\": 2,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 0.95,\n",
        "        \"clip_eps\": 0.2,\n",
        "        \"ent_coef\": 0.01,\n",
        "        \"vf_coef\": 0.5,\n",
        "        \"max_grad_norm\": 0.5,\n",
        "        \"env_name\": \"RobotWarehouse-v0\",\n",
        "        \"num_eval_episodes\": 32, # Number of episodes for evaluation.\n",
        "        \"num_evaluation\": 30, # Number of evaluation runs.\n",
        "        \"evaluation_greedy\": False, # Whether to use a greedy policy during evaluation.\n",
        "        \"seed\":42\n",
        "    }"
      ],
      "metadata": {
        "id": "wexJ0Slr8INC"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Experiment"
      ],
      "metadata": {
        "id": "OTMzsQEOa4Bv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we train MAPPO on `small-4ag-easy` scenarion from RobotWarehouse"
      ],
      "metadata": {
        "id": "SOMJZaDGbx8P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run experiment\n",
        "trained_params=run_experiment(config)\n",
        "print(f\"{Fore.CYAN}{Style.BRIGHT}MAPPO experiment completed{Style.RESET_ALL}\")"
      ],
      "metadata": {
        "id": "QsgzhCE0_DJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's render one episode using the trained system"
      ],
      "metadata": {
        "id": "Y09oBv9cczeO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "render_one_episode(config, trained_params, 42)"
      ],
      "metadata": {
        "id": "lMSKw2_q8YHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython\n",
        "from IPython.display import Image\n",
        "Image(filename='/content/rware.gif',embed=True)"
      ],
      "metadata": {
        "id": "Xq_HhYIrWOWK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
